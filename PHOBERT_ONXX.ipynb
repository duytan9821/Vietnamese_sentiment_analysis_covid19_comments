{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28634,"status":"ok","timestamp":1653970309601,"user":{"displayName":"Tân Phạm Duy","userId":"08992204997386358561"},"user_tz":-420},"id":"jwzX3rGC1ske","outputId":"d07a5af5-1f11-4431-9e1c-30787beb66d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["#Kết nối với drive cá nhân\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"V098b8uwdfNp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 35.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml\u003e=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 67.6 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,\u003c0.13,\u003e=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.1.0-\u003etransformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003etransformers) (3.0.9)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003etransformers) (3.8.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (3.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2022.5.18.1)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-\u003etransformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPOAPMCrdkJa"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gzYRV3LN-G8"},"outputs":[],"source":["!pip install vncorenlp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rzC4SGMDOASO"},"outputs":[],"source":["!mkdir -p vncorenlp/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv VnCoreNLP-1.1.1.jar vncorenlp/\n","!mv vi-vocab vncorenlp/models/wordsegmenter/\n","!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSXEDEwAdbxy"},"outputs":[],"source":["import os\n","from sklearn.metrics import classification_report\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from transformers import TFAutoModel, AutoTokenizer\n","import transformers\n","from tqdm.notebook import tqdm\n","from tokenizers import BertWordPieceTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99uGK0qHNW_V"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vSla6yiOIM8"},"outputs":[],"source":["from vncorenlp import VnCoreNLP\n","vncorenlp = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8jOpdUWNcGk"},"outputs":[],"source":["import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-tSUH5ix6Ct"},"outputs":[],"source":["from sklearn.model_selection import train_test_split "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6abFnarOyEbp"},"outputs":[],"source":["import pandas as pd\n","data = pd.read_excel(\"/content/gdrive/MyDrive/Luan_van/data_crawl_19.3.xlsx\", index_col=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owKpieTSRD5y"},"outputs":[],"source":["data_Sentiment = data['Sentiment']\n","data_Sentiment = data_Sentiment.astype(int)\n","data['Sentiment'] = data_Sentiment\n","data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25Snt8OdyLMj"},"outputs":[],"source":["train, test= train_test_split(data, test_size=0.3, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1n1NUfW2FQo"},"outputs":[],"source":["X_train = train['Text']\n","y_train = train['Sentiment'].values\n","X_test = test['Text']\n","y_test = test['Sentiment'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJZQYwACNrlL"},"outputs":[],"source":["#pre-process\n","import re\n","import numpy as np\n","\n","def deEmojify(text):\n","    regrex_pattern = re.compile(pattern = \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols \u0026 pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport \u0026 map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           \"]+\", flags = re.UNICODE)\n","    return regrex_pattern.sub(r'',text)\n","\n","def preprocess(text, tokenized=True, lowercased=True):\n","    # text = ViTokenizer.tokenize(text)\n","    # text = ' '.join(vncorenlp.tokenize(text)[0])\n","    #text = filter_stop_words(text, stopwords)\n","    text = deEmojify(text)\n","    text = text.lower() if lowercased else text\n","    if tokenized:\n","        pre_text = \"\"\n","        sentences = vncorenlp.tokenize(text)\n","        for sentence in sentences:\n","            pre_text += \" \".join(sentence)\n","        text = pre_text\n","    return text\n","\n","def pre_process_features(X, y, tokenized=True, lowercased=True):\n","    X = [preprocess(str(p), tokenized=tokenized, lowercased=lowercased) for p in list(X)]\n","    for idx, ele in enumerate(X):\n","        if not ele:\n","            np.delete(X, idx)\n","            np.delete(y, idx)\n","    return X, y\n","def preprocess_phobert(text, tokenized=True, lowercased=True):\n","    # text = ViTokenizer.tokenize(text)\n","    # text = ' '.join(vncorenlp.tokenize(text)[0])\n","    #text = filter_stop_words(text, stopwords)\n","    text = deEmojify(text)\n","    text = text.lower() if lowercased else text\n","    if tokenized:\n","        pre_text = \"\"\n","        sentences = vncorenlp.tokenize(text)\n","        for sentence in sentences:\n","            pre_text += \" \".join(sentence)\n","        text = pre_text\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPHR7bLGzu0V"},"outputs":[],"source":["max_length = 256\n","\n","def norm(x):\n","    if len(x[0]) \u003c 200:\n","        temp = torch.cat((x[0], torch.tensor([1]*(200-len(x[0])))), 0)\n","    else:\n","        temp = torch.tensor([x[0][:200].tolist()])\n","    return temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juycbvNatR0g"},"outputs":[],"source":["import torch\n","from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n","from sklearn.metrics import classification_report\n","model = AutoModelForSequenceClassification.from_pretrained(\"/content/gdrive/MyDrive/Luan_van/phobert\", num_labels = 3)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\",use_fast=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSc6b8N-_X5_"},"outputs":[],"source":["#dư đoán nhãn\n","import torch.nn.functional as F\n","import time\n","\n","texts = \"Không nên tiêm vaccine cho người lớn\"\n","texts = preprocess_phobert(texts)\n","\n","\n","tokenized_texts = tokenizer.encode_plus(texts, add_special_tokens = True, max_length=200,truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n","print(texts)\n","start = time.time()\n","y_pred_classify2 = model(tokenized_texts['input_ids'])\n","y_pred = torch.argmax(y_pred_classify2.logits)\n","end  = time.time()\n","t = end - start\n","print(t)\n","if y_pred == 2:\n","  print(\"positive\" )\n","if y_pred == 1:\n","  print(\"neutral\" )\n","if y_pred == 0:\n","  print(\"negative\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7eZm6cs5OCS"},"outputs":[],"source":["tokenized_texts['input_ids']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9zIQoWYo8XU"},"outputs":[],"source":["print(tokenized_texts['input_ids'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glpSxvALtSSO"},"outputs":[],"source":["batch_size = 4\n","x = torch.tensor([[1]*200]*batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AF8-T1Bytiik"},"outputs":[],"source":["torch_out = model(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PjWfgOcEtq0-"},"outputs":[],"source":["torch.onnx.export(model,              \n","                  x,\n","                  \"model.onnx\",  \n","                  export_params=True,       \n","                  opset_version=11,         \n","                  do_constant_folding=True,  \n","                  input_names = ['input'],   \n","                  output_names = ['output'], \n","                  dynamic_axes={'input' : {0 : '-1'},    \n","                                'output' : {0 : '-1'}})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2WQE0vkvepp"},"outputs":[],"source":["!pip install onnx #Open Neural Network Exchange\n","import onnx\n","\n","onnx_model = onnx.load(\"model.onnx\")\n","onnx.checker.check_model(onnx_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBgPdT8WvmI1"},"outputs":[],"source":["!pip install onnxruntime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPoho-2YvyxF"},"outputs":[],"source":["import onnxruntime\n","\n","ort_session = onnxruntime.InferenceSession(\"model.onnx\")\n","\n","def to_numpy(tensor):\n","    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n","import time\n","start = time.time()\n","# compute ONNX Runtime output prediction\n","ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tokenized_texts['input_ids'])}\n","ort_outs = ort_session.run(None, ort_inputs)\n","end = time.time()\n","print(end - start)\n","# compare ONNX Runtime and PyTorch results\n","print(\"Exported model has been\") "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"PHOBERT_ONXX.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}